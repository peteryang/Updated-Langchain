{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Health Insurance Coverage Status and Type \\nby Geography: 2021 and 2022\\nAmerican Community Survey Briefs\\nACSBR-015Issued September 2023Douglas Conway and Breauna Branch\\nINTRODUCTION\\nDemographic shifts as well as economic and govern-\\nment policy changes can affect peopleâ€™s access to health coverage. For example, between 2021 and 2022, the labor market continued to improve, which may have affected private coverage in the United States \\nduring that time.\\n1 Public policy changes included \\nthe renewal of the Public Health Emergency, which \\nallowed Medicaid enrollees to remain covered under the Continuous Enrollment Provision.\\n2 The American \\nRescue Plan (ARP) enhanced Marketplace premium subsidies for those with incomes above 400 percent of the poverty level as well as for unemployed people.\\n3', metadata={'source': 'us_census/acsbr-015.pdf', 'page': 0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read the ppdfs from the folder\n",
    "loader=PyPDFDirectoryLoader(\"./us_census\")\n",
    "\n",
    "documents=loader.load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "\n",
    "final_documents=text_splitter.split_documents(documents)\n",
    "final_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Embedding Using Huggingface\n",
    "huggingface_embeddings=HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",      #sentence-transformers/all-MiniLM-l6-v2\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.46568421e-02 -1.19099468e-02 -3.37892435e-02  2.94559337e-02\n",
      "  5.19159697e-02  5.73839732e-02 -4.10017371e-02  2.74268053e-02\n",
      " -1.05128221e-01 -1.58056170e-02  7.94858858e-02  5.64318374e-02\n",
      " -1.31765548e-02 -3.41543928e-02  5.81606617e-03  4.72547784e-02\n",
      " -1.30746728e-02  3.12992837e-03 -3.44225802e-02  3.08406297e-02\n",
      " -4.09086272e-02  3.52737941e-02 -2.43761707e-02 -4.35831435e-02\n",
      "  2.41503250e-02  1.31986802e-02 -4.84447367e-03  1.92348063e-02\n",
      " -5.43912798e-02 -1.42735049e-01  5.15527558e-03  2.93115806e-02\n",
      " -5.60810976e-02 -8.53534043e-03  3.14141326e-02  2.76736487e-02\n",
      " -2.06188411e-02  8.24231580e-02  4.15425338e-02  5.79655431e-02\n",
      " -3.71587090e-02  6.26161555e-03 -2.41390038e-02 -5.61794639e-03\n",
      " -2.51715258e-02  5.04965847e-03 -2.52801199e-02 -2.91943667e-03\n",
      " -8.24045856e-03 -5.69604412e-02  2.30822992e-02 -5.54217491e-03\n",
      "  5.11555709e-02  6.09937720e-02  6.49766028e-02 -5.38514294e-02\n",
      "  2.19109710e-02 -2.54194345e-02 -4.49223258e-02  4.22458909e-02\n",
      "  4.75251973e-02  7.23241945e-04 -2.61084497e-01  9.30173174e-02\n",
      "  1.13597531e-02  4.90669236e-02 -1.06286863e-02 -8.08732025e-03\n",
      " -1.53562250e-02 -5.33786044e-02 -6.89966753e-02  4.75178249e-02\n",
      " -5.68596087e-02  9.38640349e-03  4.24065813e-02  2.54346300e-02\n",
      "  9.67098027e-03  7.90798478e-03  2.25161146e-02  1.91008695e-03\n",
      "  3.06091849e-02  2.43992228e-02 -1.34115629e-02 -4.77401018e-02\n",
      "  4.89940085e-02 -9.49416384e-02  5.62893786e-02 -4.76260856e-02\n",
      "  2.81447247e-02 -2.54329592e-02 -3.84951420e-02  1.00939898e-02\n",
      "  1.90574967e-04  3.36625651e-02  1.00181764e-02  2.83524077e-02\n",
      " -2.68965703e-03 -6.96364464e-03 -3.54914442e-02  3.42758864e-01\n",
      " -1.94496736e-02  1.43988272e-02 -5.68815926e-03  1.71481054e-02\n",
      " -2.88607227e-03 -5.81653006e-02  6.35174278e-04  5.17302472e-03\n",
      "  2.06331499e-02  1.65708363e-02  2.15096287e-02 -2.38796212e-02\n",
      "  2.89275497e-02  4.67318855e-02 -3.56105007e-02 -1.05078993e-02\n",
      "  3.70704755e-02  1.57502573e-02  9.43095461e-02 -2.50715390e-02\n",
      " -9.55966488e-03  1.78565644e-02 -9.41781700e-03 -4.57858853e-02\n",
      "  1.82930920e-02  5.81431910e-02  4.94311452e-02  1.46350682e-01\n",
      "  2.16058046e-02 -3.92896272e-02  1.03241272e-01 -3.48300152e-02\n",
      " -6.61874190e-03  7.07989139e-03  9.26972541e-04  4.49864147e-03\n",
      " -2.89777350e-02  4.02418897e-02 -5.23192668e-03  4.59961966e-02\n",
      "  4.23971657e-03 -4.83791903e-03 -3.23239272e-03 -1.41072914e-01\n",
      " -3.76811326e-02  1.83623895e-01 -2.96609253e-02  4.90660481e-02\n",
      "  3.90551575e-02 -1.57757830e-02 -3.86351235e-02  4.65631112e-02\n",
      " -2.43485831e-02  3.57695259e-02 -3.54947336e-02  2.36265529e-02\n",
      " -3.42005253e-04  3.11703756e-02 -2.39356291e-02 -5.94757795e-02\n",
      "  6.06259182e-02 -3.81902382e-02 -7.04255253e-02  1.42479818e-02\n",
      "  3.34432311e-02 -3.85254920e-02 -1.71951316e-02 -7.12288544e-02\n",
      "  2.64976248e-02  1.09496014e-02  1.32650556e-02  3.89527939e-02\n",
      "  1.60355456e-02 -3.17629948e-02  1.02013737e-01  2.92912088e-02\n",
      " -2.29205750e-02 -8.38050991e-03 -1.72173008e-02 -6.78820759e-02\n",
      "  5.39417984e-03 -2.32347213e-02 -6.07406721e-02 -3.86575572e-02\n",
      " -1.54306814e-02 -3.84983607e-02 -5.02867699e-02  5.04235066e-02\n",
      "  4.94898371e-02 -1.41083607e-02 -2.98142829e-03  9.76448719e-05\n",
      " -6.59190714e-02  3.01006772e-02 -5.46608877e-04 -1.64787564e-02\n",
      " -5.21614812e-02 -3.30224936e-03  4.75748293e-02 -3.40808481e-02\n",
      " -2.98659988e-02  2.75014769e-02  5.90203376e-03 -2.64037005e-03\n",
      " -1.61242820e-02  2.05222666e-02  1.21104913e-02 -5.49781695e-02\n",
      "  5.10389581e-02 -7.92088918e-03  7.25202309e-03  3.51750813e-02\n",
      "  3.66276689e-02  5.67718002e-04  2.60788519e-02  2.50971187e-02\n",
      "  1.14481244e-02 -2.54925136e-02  1.96417850e-02  2.84220055e-02\n",
      "  2.82554105e-02  6.57489672e-02  9.26553905e-02 -2.68629670e-01\n",
      " -8.90575000e-04  3.16916895e-03  5.08354930e-03 -6.42101020e-02\n",
      " -4.56614643e-02 -4.62259948e-02  3.60924900e-02  8.29056278e-03\n",
      "  8.92349333e-02  5.68022318e-02  6.91056484e-03 -1.08684218e-02\n",
      "  9.36060473e-02  1.03680575e-02 -8.60929713e-02  1.77331977e-02\n",
      " -2.00802572e-02 -1.85124837e-02  5.62429603e-04 -9.38339625e-03\n",
      "  7.76060671e-03 -5.37273698e-02 -2.30028033e-02  7.48891011e-02\n",
      " -1.29692759e-02  6.53716773e-02 -4.24982980e-02 -7.10293651e-02\n",
      " -1.56803392e-02 -6.23028502e-02  5.36034219e-02 -6.53214939e-03\n",
      " -1.15985490e-01  6.70967996e-02  1.93367098e-02 -6.67827800e-02\n",
      " -2.01756856e-03 -6.27636835e-02 -2.95005292e-02 -2.71986593e-02\n",
      "  4.49796766e-02 -6.61587268e-02  2.13751215e-02 -2.94077676e-02\n",
      " -5.71503006e-02  4.05282304e-02  7.11039379e-02 -6.80165589e-02\n",
      "  2.11908463e-02  1.30515229e-02 -2.91152596e-02 -2.25581452e-02\n",
      " -1.60188396e-02  3.20553891e-02 -5.89460284e-02 -2.97131911e-02\n",
      "  3.42681482e-02 -1.58376303e-02 -9.31769144e-03  3.59834395e-02\n",
      "  3.65338055e-03  4.73319478e-02 -1.06235212e-02 -8.69737193e-03\n",
      " -4.38009948e-02  5.94559172e-03 -2.41493434e-02 -7.79940486e-02\n",
      "  1.46542592e-02  1.05613749e-02  5.45365550e-02 -3.17897052e-02\n",
      " -1.26762986e-02  7.92556442e-03 -1.38133084e-02  5.01396917e-02\n",
      " -7.28576351e-03 -5.23703592e-03 -5.32640778e-02  4.78208698e-02\n",
      " -5.38353510e-02  1.11437673e-02  3.96673828e-02 -1.93496421e-02\n",
      "  9.94820613e-03 -3.53480224e-03  3.58560332e-03 -9.61500499e-03\n",
      "  2.15323791e-02 -1.82350241e-02 -2.15188786e-02 -1.38835758e-02\n",
      " -1.76699050e-02  3.37992969e-04 -3.84836458e-04 -2.25800499e-01\n",
      "  4.51243334e-02  1.53376935e-02 -1.76967625e-02 -1.42526561e-02\n",
      " -7.00284680e-03 -3.13724987e-02  2.13673245e-03 -9.28347465e-03\n",
      " -1.66987143e-02  4.66264412e-02  7.71809146e-02  1.26696974e-01\n",
      " -1.83595698e-02 -1.39636770e-02 -1.23305700e-03  5.93339317e-02\n",
      " -1.37459370e-03  1.98233444e-02 -2.92635635e-02  4.96656895e-02\n",
      " -6.07207231e-02  1.53544769e-01 -4.67309393e-02  1.97029021e-02\n",
      " -7.67833367e-02 -7.73232337e-03  3.71618755e-02 -3.00591141e-02\n",
      "  8.30264017e-03  2.06259079e-02  1.97463995e-03  3.39764208e-02\n",
      " -1.70869436e-02  4.84796092e-02  1.20781930e-02  1.24999108e-02\n",
      "  5.61724454e-02  9.88544710e-03  2.13878993e-02 -4.25293520e-02\n",
      " -1.94037072e-02  2.47837864e-02  1.37260938e-02  6.41119555e-02\n",
      " -2.84480695e-02 -4.64116409e-02 -5.36255538e-02 -6.95267881e-05\n",
      "  6.45710081e-02 -4.32055152e-04 -1.32470671e-02  5.85135445e-03\n",
      "  1.48595655e-02 -5.41847721e-02 -2.02038419e-02 -5.98262586e-02\n",
      "  3.67028899e-02  1.43320882e-03 -8.64466373e-03  2.90671699e-02\n",
      "  4.38365936e-02 -7.64942765e-02  1.55718094e-02  6.65831119e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorStore Creation\n",
    "vectorstore=FAISS.from_documents(final_documents[:120],huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 U.S. Census Bureau\n",
      "WHAT IS HEALTH INSURANCE COVERAGE?\n",
      "This brief presents state-level estimates of health insurance coverage \n",
      "using data from the American Community Survey (ACS). The  \n",
      "U.S. Census Bureau conducts the ACS throughout the year; the \n",
      "survey asks respondents to report their coverage at the time of \n",
      "interview. The resulting measure of health insurance coverage, \n",
      "therefore, reflects an annual average of current comprehensive \n",
      "health insurance coverage status.* This uninsured rate measures a \n",
      "different concept than the measure based on the Current Population \n",
      "Survey Annual Social and Economic Supplement (CPS ASEC). \n",
      "For reporting purposes, the ACS broadly classifies health insurance \n",
      "coverage as private insurance or public insurance. The ACS defines \n",
      "private health insurance as a plan provided through an employer \n",
      "or a union, coverage purchased directly by an individual from an \n",
      "insurance company or through an exchange (such as healthcare.\n"
     ]
    }
   ],
   "source": [
    "## Query using Similarity Search\n",
    "query=\"WHAT IS HEALTH INSURANCE COVERAGE?\"\n",
    "relevant_docments=vectorstore.similarity_search(query)\n",
    "\n",
    "print(relevant_docments[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x76ce6e4912d0> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hugging Face Hub is an platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/myenv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1 (Request ID: cVO8sX_SfCRliFSfZvt_N)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m hf\u001b[38;5;241m=\u001b[39mHuggingFaceHub(\n\u001b[1;32m      4\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m500\u001b[39m}\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the health insurance coverage?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    288\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     ]\n\u001b[0;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/langchain_core/language_models/llms.py:1317\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1316\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1317\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1319\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/langchain_community/llms/huggingface_hub.py:135\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    133\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 135\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/huggingface_hub/inference/_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[0;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/conda/envs/myenv/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1 (Request ID: cVO8sX_SfCRliFSfZvt_N)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
    "\n",
    ")\n",
    "query=\"What is the health insurance coverage?\"\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "llm = hf \n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answers:\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA=RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"DIFFERENCES IN THE\n",
    "UNINSURED RATE BY STATE\n",
    "IN 2022\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
